#!/usr/bin/env python


import argparse
import multiprocessing
import pathlib

import numpy as np

from discrete_shocklets import shocklets, kernel_functions, weighting_functions, utils


def parse_args():
    parser = argparse.ArgumentParser(
        description='Runs the Shocklet Transform And Ranking (STAR) algorithm on data',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        '-i',
        '--input',
        type=validate_input_dir,
        help='Path to files on which to run the algorithm. '
             'The files should be in row-major order. '
             'That is, they should be a N_variable x T matrix, where T is the number of time steps. '
             'This algorithm multi-processes over files so if your time series are very long it could '
             'be significantly faster to break them up into multiple files, one time series per file. '
             '(This is true only if you have many cores on which processing can occur in parallel.)',
        default=pathlib.Path(),
    )
    parser.add_argument(
        '-e',
        '--ending',
        type=str,
        help='Ending of files on which to run algorithm. Must be readable to numpy.genfromtxt()',
        default='csv'
    )
    parser.add_argument(
        '-d',
        '--delimiter',
        type=str,
        help='Delimiter of entries in files',
        default=',',
    )
    parser.add_argument(
        '-o',
        '--output',
        type=validate_output_dir,
        help='Path to which to save output',
        default=pathlib.Path('../out'),
    )
    parser.add_argument(
        '-k',
        '--kernel',
        type=lambda x: getattr(kernel_functions, x),
        help='Kernel function used by STAR to identify interesting regimes. '
             'Most common options are: '
             'haar, the Haar wavelet, which looks for pure level changes; '
             'power_zero_cusp, which is the building block for other cusp kernels and looks for power '
             '(monomial) growth followed by an abrupt drop to constant low levels; '
             'power_cusp, which is a power cusp shape; '
             'exp_zero_cusp, which is like power_zero_cusp except with exponential growth; '
             'and exp_cusp, which is like power_cusp except with exponential growth. '
             'Combining these kernels with a reflection (see the -r option) '
             'is probably enough to look for most interesting behavior. '
             'See kernel_functions.py for additional options and details.',
        default=kernel_functions.power_cusp,
        choices={k.__name__ for k in kernel_functions.registered_kernel_functions}
    )
    parser.add_argument(
        '-r',
        '--reflection',
        type=lambda x: int(x) % 4,
        help='Element of the reflection group R_4 to use. Computed mod 4.',
        default=0,
    )
    parser.add_argument(
        '-b',
        '--bvalue',
        type=float,
        help='Multiplier for std dev in classification.',
        default=0.75,
    )
    parser.add_argument(
        '-g',
        '--geval',
        type=float,
        help='Threshold for window construction.',
        default=0.5,
    )
    parser.add_argument(
        '-l',
        '--lookback',
        type=int,
        help='Number of indices to look back for window construction.',
        default=0,
    )
    parser.add_argument(
        '-w',
        '--weighting',
        type=lambda x: getattr(weighting_functions, x),
        help='Method for weighting of cusp indicator functions. '
             'max_change uses the dynamic range of the original series within each window. '
             'max_rel_change computes max_change on the array of log returns of the original time series.',
        default=weighting_functions.max_change,
        choices={x.__name__ for x in weighting_functions.registered_weighting_functions},
    )
    parser.add_argument(
        '-wmin',
        '--wmin',
        type=int,
        help='Smallest kernel size.',
        default=10,
    )
    parser.add_argument(
        '-wmax',
        '--wmax',
        type=int,
        help='Largest kernel size. Defaults to min{500, 1/2 length of time series}.',
        default=None,
    )
    parser.add_argument(
        '-nw',
        '--nw',
        type=int,
        help='Number of kernels to use. Ideally (wmax - wmin) / nw would be an integer.',
        default=100,
    )
    parser.add_argument(
        '-s',
        '--savespec',
        type=lambda x: x.strip().lower(),
        help='Spec for saving. Options are: '
             'cc, to save cusplet transform; '
             'indic, to save indicator function;'
             'windows, to save anomalous windows; '
             'weighted, to save weighted indicator function; '
             'all, to save everything. '
             'Files are saved in the numpy compressed archive format (.npz files).',
        default='all',
        choices={'cc', 'indic', 'windows', 'weighted', 'all'},
    )
    parser.add_argument(
        '-norm',
        '--norm',
        type=lambda x: x.lower() in {'t', 'true', '1'},
        help='Whether or not to normalize series to be wide-sense stationary '
             'with inter-temporal zero mean and unit variance.',
        default=False,
    )

    return parser.parse_known_args()


def validate_input_dir(path):
    input_dir = pathlib.Path(path)
    if not input_dir.is_dir():
        FileNotFoundError(f'{path} does not exist or is not a directory')
    return input_dir


def validate_output_dir(path):
    output_dir = pathlib.Path(path)
    output_dir.mkdir(exist_ok=True, parents=True)
    return output_dir


def _process(
        data,
        kernel,
        reflection,
        wmin,
        wmax,
        nw,
        kernel_args,
        outdir,
        weighting,
        savespec,
        b,
        geval,
        nback,
        orig_fname,
        norm
):
    """Computes the shocklet (cusplet) transform on time series data.

    Computes the transform on each row of the passed data.
    The collection of cusplet transforms will be of shape (data.shape[0], nw, data.shape[1]).
    """
    if len(data.shape) < 2:
        data = data.reshape(1, data.shape[0])

    if wmax is None:
        nt = data.shape[1]
        wmax = min(500, int(0.5 * nt))

    widths = np.linspace(wmin, wmax, nw).astype(int)

    for i, row in enumerate(data):
        if norm:
            row = utils.normalize(row)
        try:
            cc, _ = shocklets.cusplet(row, widths, kernel_func=kernel, kernel_args=kernel_args, reflection=reflection)
        except Exception as e:
            print(f'Error occurred in computation of shocklet transform of {orig_fname}')
            print(f'Error: {e}')
            return

        if savespec == 'cc':
            np.savez_compressed(
                outdir / f'{orig_fname}-row{i}',
                cc=cc,
            )
            return

        extrema, sum_cc, gearray = shocklets.classify_cusps(
            cc,
            b=b,
            geval=geval
        )
        if savespec == 'indic':
            np.savez_compressed(
                outdir / f'{orig_fname}-row{i}',
                indic=sum_cc,
            )
            return

        windows = shocklets.make_components(
            gearray,
            scan_back=nback
        )
        if savespec == 'windows':
            np.savez_compressed(
                outdir / f'{orig_fname}-row{i}',
                windows=windows,
            )
            return

        weighted_sumcc = np.copy(sum_cc)
        for window in windows:
            weighted_sumcc[window] *= weighting(row[window])

        if savespec == 'weighted':
            np.savez_compressed(
                outdir / f'{orig_fname}-row{i}',
                weighted_indic=weighted_sumcc,
            )
        if savespec == 'all':
            np.savez_compressed(
                outdir / f'{orig_fname}-row{i}',
                cc=cc,
                indic=sum_cc,
                windows=windows,
                weighted_indic=weighted_sumcc,
            )


def _mp_process(fname, args, kernel_args):
    if args.delimiter == 'none':
        data = np.genfromtxt(fname)
    else:
        data = np.genfromtxt(fname, delimiter=args.delimiter)

    _process(
        data,
        args.kernel,
        args.reflection,
        args.wmin,
        args.wmax,
        args.nw,
        kernel_args,
        args.output,
        args.weighting,
        args.savespec,
        args.bvalue,
        args.geval,
        args.lookback,
        fname.stem,
        args.norm
    )


def main():
    args, kernel_args = parse_args()

    try:
        kernel_args_ = []
        for i, arg in enumerate(kernel_args):
            kernel_args_.append(float(arg))
        kernel_args = kernel_args_
    except ValueError:
        ValueError(f'Kernel argument {i} = {arg} can not be safely cast to float.')

    if (args.wmax is not None) and (args.wmin >= args.wmax):
        ValueError(f'wmin ({args.wmin}) must be less than wmax ({args.wmax}).')

    fnames = sorted(args.input.glob(f'*.{args.ending}'))
    if len(fnames) == 0:
        FileNotFoundError(f'There are no files with ending {args.ending} in {args.input}')

    elif len(fnames) == 1:
        _mp_process(
            fnames[0],
            args,
            kernel_args,
        )

    elif len(fnames) > 1:
        with multiprocessing.Pool() as pool:
            pool.starmap(
                _mp_process,
                zip(
                    fnames,
                    (args for _ in range(len(fnames))),
                    (kernel_args for _ in range(len(fnames)))
                )
            )


if __name__ == "__main__":
    main()
